{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression Model on Titanic Survival Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVl9ija4vJYg+yS06mWZIZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevanshParmar/Data-Science-Summer-Camp-2021/blob/main/Logistic%20Regression%20Model%20on%20Titanic%20Survival%20Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW9450se8PQO"
      },
      "source": [
        "## **Logistic Regression Model on Titanic Survival Dataset**\n",
        "This is an implementation of the Logistic Regression Model, one of the most basic machine learning model on the Titanic survival dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipq0tEygNrzX"
      },
      "source": [
        "#### **Uploads**\n",
        "Setting up libraries and uploading dataset files.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUeghG6C-1jc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOvOFm4wB9MI",
        "outputId": "2321ac94-058b-4cd4-a440-c59b0d9e3687"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "%cd /content/gdrive/My Drive/Kaggle\n",
        "!kaggle competitions download -c titanic\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/Kaggle\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "test.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "gender_submission.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "gender_submission.csv  kaggle.json  test.csv  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab1WQhEkEWFk",
        "outputId": "3a19aba6-e0ad-4ae3-caef-09d2eb76d0b7"
      },
      "source": [
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open *.zip, *.zip.zip or *.zip.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahtoTgVyOItW"
      },
      "source": [
        "#### **Data Preprocessing**\n",
        "Constructing a dataframe and making necessary changes, such as deleting PassId and converting Sex to male=1, female=0 objective case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Moc5MJu-1nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac085dc-ac32-44b3-b984-62c029b96ba5"
      },
      "source": [
        "print(\"Input Dataframes:\")\n",
        "data = pd.read_csv('/content/gdrive/MyDrive/Kaggle/train.csv')\n",
        "col_names1 = ['PassId', 'Survived', 'PClass', 'Name', 'Sex', 'Age', 'SibSp', 'ParCh', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
        "data.columns = col_names1\n",
        "print(data.head())\n",
        "print(\" \")\n",
        "\n",
        "test = pd.read_csv('/content/gdrive/MyDrive/Kaggle/test.csv')\n",
        "col_names2 = ['PassId', 'PClass', 'Name', 'Sex', 'Age', 'SibSp', 'ParCh', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
        "test.columns = col_names2\n",
        "print(test.head())\n",
        "print(\" \")\n",
        "\n",
        "test_survived = pd.read_csv('/content/gdrive/MyDrive/Kaggle/gender_submission.csv')\n",
        "test_survived.columns = ['PassId', 'Survived']\n",
        "print(test_survived.head())\n",
        "print(\" \")\n",
        "\n",
        "print(\"Output Dataframes:\")\n",
        "data.pop(\"PassId\")\n",
        "data.pop(\"Name\")\n",
        "data.pop(\"Ticket\")\n",
        "data.pop(\"Cabin\")\n",
        "data.pop(\"Embarked\")\n",
        "ifmale = pd.get_dummies(data['Sex'], drop_first = True)\n",
        "data = pd.concat([data, ifmale], axis = 1)\n",
        "data.pop(\"Sex\")\n",
        "print(data.head())\n",
        "print(\" \")\n",
        "\n",
        "test.pop(\"PassId\")\n",
        "test.pop(\"Name\")\n",
        "test.pop(\"Ticket\")\n",
        "test.pop(\"Cabin\")\n",
        "test.pop(\"Embarked\")\n",
        "ifmale = pd.get_dummies(test['Sex'], drop_first = True)\n",
        "test = pd.concat([test, ifmale], axis = 1)\n",
        "test.pop(\"Sex\")\n",
        "test_survived.pop(\"PassId\")\n",
        "test['Survived'] = test_survived.values\n",
        "print(test.head())\n",
        "print(\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Dataframes:\n",
            "   PassId  Survived  PClass  ...     Fare Cabin  Embarked\n",
            "0       1         0       3  ...   7.2500   NaN         S\n",
            "1       2         1       1  ...  71.2833   C85         C\n",
            "2       3         1       3  ...   7.9250   NaN         S\n",
            "3       4         1       1  ...  53.1000  C123         S\n",
            "4       5         0       3  ...   8.0500   NaN         S\n",
            "\n",
            "[5 rows x 12 columns]\n",
            " \n",
            "   PassId  PClass  ... Cabin Embarked\n",
            "0     892       3  ...   NaN        Q\n",
            "1     893       3  ...   NaN        S\n",
            "2     894       2  ...   NaN        Q\n",
            "3     895       3  ...   NaN        S\n",
            "4     896       3  ...   NaN        S\n",
            "\n",
            "[5 rows x 11 columns]\n",
            " \n",
            "   PassId  Survived\n",
            "0     892         0\n",
            "1     893         1\n",
            "2     894         0\n",
            "3     895         0\n",
            "4     896         1\n",
            " \n",
            "Output Dataframes:\n",
            "   Survived  PClass   Age  SibSp  ParCh     Fare  male\n",
            "0         0       3  22.0      1      0   7.2500     1\n",
            "1         1       1  38.0      1      0  71.2833     0\n",
            "2         1       3  26.0      0      0   7.9250     0\n",
            "3         1       1  35.0      1      0  53.1000     0\n",
            "4         0       3  35.0      0      0   8.0500     1\n",
            " \n",
            "   PClass   Age  SibSp  ParCh     Fare  male  Survived\n",
            "0       3  34.5      0      0   7.8292     1         0\n",
            "1       3  47.0      1      0   7.0000     0         1\n",
            "2       2  62.0      0      0   9.6875     1         0\n",
            "3       3  27.0      0      0   8.6625     1         0\n",
            "4       3  22.0      1      1  12.2875     0         1\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOuAbyTaR49Z"
      },
      "source": [
        "Filling NaN Age values with median age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckb4d_I3_B1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c04b501e-591e-4ff5-93a9-f5fe2f410c1e"
      },
      "source": [
        "data['Age'] = data['Age'].fillna(data['Age'].median())\n",
        "test['Age'] = test['Age'].fillna(test['Age'].median())\n",
        "print(data)\n",
        "print(\" \")\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Survived  PClass   Age  SibSp  ParCh     Fare  male\n",
            "0           0       3  22.0      1      0   7.2500     1\n",
            "1           1       1  38.0      1      0  71.2833     0\n",
            "2           1       3  26.0      0      0   7.9250     0\n",
            "3           1       1  35.0      1      0  53.1000     0\n",
            "4           0       3  35.0      0      0   8.0500     1\n",
            "..        ...     ...   ...    ...    ...      ...   ...\n",
            "886         0       2  27.0      0      0  13.0000     1\n",
            "887         1       1  19.0      0      0  30.0000     0\n",
            "888         0       3  28.0      1      2  23.4500     0\n",
            "889         1       1  26.0      0      0  30.0000     1\n",
            "890         0       3  32.0      0      0   7.7500     1\n",
            "\n",
            "[891 rows x 7 columns]\n",
            " \n",
            "     PClass   Age  SibSp  ParCh      Fare  male  Survived\n",
            "0         3  34.5      0      0    7.8292     1         0\n",
            "1         3  47.0      1      0    7.0000     0         1\n",
            "2         2  62.0      0      0    9.6875     1         0\n",
            "3         3  27.0      0      0    8.6625     1         0\n",
            "4         3  22.0      1      1   12.2875     0         1\n",
            "..      ...   ...    ...    ...       ...   ...       ...\n",
            "413       3  27.0      0      0    8.0500     1         0\n",
            "414       1  39.0      0      0  108.9000     0         1\n",
            "415       3  38.5      0      0    7.2500     1         0\n",
            "416       3  27.0      0      0    8.0500     1         0\n",
            "417       3  27.0      1      1   22.3583     1         0\n",
            "\n",
            "[418 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnLzf9GZSFyA"
      },
      "source": [
        "Sending dataframe to NumPy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVyypGJf_Bz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d485bc-6a1b-4b4d-e23f-2ef69f258342"
      },
      "source": [
        "input_train = data[['PClass', 'Age', 'SibSp', 'ParCh', 'Fare', 'male']].to_numpy()\n",
        "output_train = data['Survived'].to_numpy()\n",
        "print(input_train.shape)\n",
        "print(input_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(891, 6)\n",
            "(418, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlHfSE1dSOoq"
      },
      "source": [
        "#### **Modeling**\n",
        "In the next blocks, we define, train and optimize our model.\n",
        "1. The first block defines the sigmoid function, necessary for logistic regression.\n",
        "2. The second block defines the optimization function.\n",
        "3. The third block puts up the inital parameters as 0.\n",
        "4. The fourth block runs the training sessions of the model, and finallly outputs the trained theta parameters of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kQOSNy__BhD"
      },
      "source": [
        "def g(z):\n",
        "    return 1/(1+np.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltRfH0wO_Beq"
      },
      "source": [
        "def optimize(x, y, learning_rate, N_iterations, parameters):\n",
        "    size = x.shape[0] #620\n",
        "    weights = parameters[\"weights\"]                                             #theta1-6\n",
        "    bias = parameters[\"bias\"]                                                   #theta0\n",
        "    for i in range(N_iterations):\n",
        "        h = g(bias + np.dot(x, weights))                                        #h becomes the hypothesis function\n",
        "        loss = -1/size * np.sum(y * np.log(h)) + (1 - y) * np.log(1-h)          #log-likelihood\n",
        "        del_weights = 1/size * np.dot(x.T, (h-y))                               #change in weights\n",
        "        del_bias = 1/size * np.sum(h-y)                                         #change in bias\n",
        "        weights = weights - learning_rate * del_weights                         #learning\n",
        "        bias = bias - learning_rate * del_bias                                  #learning\n",
        "    \n",
        "    parameters[\"weights\"] = weights\n",
        "    parameters[\"bias\"] = bias\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hv9XF7b_RCo"
      },
      "source": [
        "initial_parameters = {}\n",
        "initial_parameters[\"weights\"] = np.zeros(input_train.shape[1])\n",
        "initial_parameters[\"bias\"] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLdhOL7r_Q_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3cb4b8-2f0a-4f98-9575-e2cc52eead1e"
      },
      "source": [
        "def train(x, y, lr, N_it):\n",
        "    return optimize(x, y, lr, N_it, initial_parameters)\n",
        "\n",
        "theta = train(input_train, output_train, lr = 0.005, N_it = 5000)\n",
        "print(theta['weights'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.10967463 -0.01136579 -0.29063817  0.02180507  0.00997821 -1.51893593]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htRK4SZmTjKK"
      },
      "source": [
        "#### **Predictions and Accuracy**\n",
        "In the next two blocks, we have measured the various statistical parameters of our model, such as accuracy, loss, F1 score, sensitivity and precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp5UaPzFVo9f"
      },
      "source": [
        "def stats(dataset):\n",
        "    z = []\n",
        "    for i in range(dataset.shape[0]):\n",
        "        p  = 0.0\n",
        "        p += theta[\"bias\"]\n",
        "        p += theta[\"weights\"][0] * dataset['PClass'][i]\n",
        "        p += theta[\"weights\"][1] * dataset['Age'][i]\n",
        "        p += theta[\"weights\"][2] * dataset['SibSp'][i]\n",
        "        p += theta[\"weights\"][3] * dataset['ParCh'][i]\n",
        "        p += theta[\"weights\"][4] * dataset['Fare'][i]\n",
        "        p += theta[\"weights\"][5] * dataset['male'][i]\n",
        "        z.insert(len(z), p)\n",
        "    #                              \n",
        "    sigmoids = []\n",
        "    for val in z:\n",
        "        sigmoids.insert(len(sigmoids), g(val))\n",
        "    #                              \n",
        "    predictions = []\n",
        "    for p in sigmoids:\n",
        "        if p >= 0.5:\n",
        "            predictions.insert(len(predictions), 1)\n",
        "        else:\n",
        "            predictions.insert(len(predictions), 0)\n",
        "    prediction = np.array(predictions)\n",
        "    survive_data = np.array(dataset['Survived'])\n",
        "    #                              \n",
        "    loss = 0\n",
        "    f_neg = 0\n",
        "    f_pos = 0 \n",
        "    t_neg = 0\n",
        "    t_pos = 0\n",
        "    #                              \n",
        "    for i, j in zip(prediction, survive_data):\n",
        "        if i == 1 and j == 1:\n",
        "            t_pos+=1\n",
        "        elif i == 1 and j == 0:\n",
        "            f_pos+=1\n",
        "            loss+=1\n",
        "        elif i==0 and j == 1:\n",
        "            f_neg+=1\n",
        "            loss+=1\n",
        "        else:\n",
        "            t_neg+=1\n",
        "    #                              \n",
        "    rec = t_pos / (t_pos + f_neg)\n",
        "    prc = t_pos / (t_pos + f_pos)\n",
        "    acc = (t_pos + t_neg) / (t_pos + t_neg + f_pos + f_neg)\n",
        "    f1s = 2 * prc * rec / (prc + rec)\n",
        "    #                              \n",
        "    print('   Accuracy is {:.2f}%'.format(100*acc))\n",
        "    print('       Loss is',loss)\n",
        "    print('   F1 Score is {:.4f}'.format(f1s))\n",
        "    print('Sensitivity is {:.4f}'.format(rec))\n",
        "    print('  Precision is {:.4f}'.format(prc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvZM5n-FYBCX",
        "outputId": "aeb4cde7-47b5-4b4a-cfc7-f2de0e3c01c1"
      },
      "source": [
        "print(\"Statistics for Training dataset are:\")\n",
        "print(\" \")\n",
        "stats(data)\n",
        "print(\" \")\n",
        "print(\" \")\n",
        "print(\"Statistics for Test dataset are:\")\n",
        "print(\" \")\n",
        "stats(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Statistics for Training dataset are:\n",
            " \n",
            "   Accuracy is 73.06%\n",
            "       Loss is 240\n",
            "   F1 Score is 0.4958\n",
            "Sensitivity is 0.3450\n",
            "  Precision is 0.8806\n",
            " \n",
            " \n",
            "Statistics for Test dataset are:\n",
            " \n",
            "   Accuracy is 77.75%\n",
            "       Loss is 93\n",
            "   F1 Score is 0.5830\n",
            "Sensitivity is 0.4276\n",
            "  Precision is 0.9155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRYQhi5b_Zm1"
      },
      "source": [
        "#### **References**\n",
        "\n",
        "1. These lecture notes were extremely helpful in understanding the mathematics of the logistic regression: https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf\n",
        "2. Many Towards Data Science (TDS) articles were helpful, especially: www.towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
        "3. Another TDS blog was a good help: www.towardsdatascience.com/optimization-loss-function-under-the-hood-part-ii-d20a239cde11\n",
        "4. This Exsilio blog was greatly helpful in visualing the final statistics of the model: www.blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/"
      ]
    }
  ]
}